{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finalproject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0tjLd13gSOzH"},"source":["# Introduction\n","\n","This notebook is for the major project submission for COMP7220/8220, on the [image/language] dataset and task.  It contains the following sections:\n","\n","*   a description of the selected conventional ML model;\n","*   some notes about the choices made in building the conventional ML model;\n","*   a description of the selected deep learning model;\n","*   some notes about the choices made in building the deep model; and\n","*   a discussion of the performance of the two models.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rZenD0ZqVAt9"},"source":["# Conventional ML Model\n","\n","The final model that produced the best-performing predictions for the Kaggle submission (accuracy 54.183%) was a linear support vector machine with C=1.0, degree=3 and gamma=auto."]},{"cell_type":"markdown","metadata":{"id":"_PQwY44H9zWo","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"_WCWQMq79zZf","colab_type":"text"},"source":["Some libraries for loading the datasets"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zJzTLEezSL_J","outputId":"286a42d9-99f7-472b-f85f-bbb14d884078","executionInfo":{"status":"ok","timestamp":1591115278500,"user_tz":-600,"elapsed":30938,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":555}},"source":["# some initialisation code\n","import numpy as np\n","from os.path import join\n","from google.colab import drive\n","import pickle\n","\n","drive.mount('/content/drive/')\n","\n","def load_pickle(path):\n","    with open(path, 'rb') as f:\n","        file = pickle.load(f)\n","        print ('Loaded %s..' %path)\n","        return file\n","\n","dataset_directory = '/content/drive/My Drive/20comp8220/proj/text_dataset/'  ## CHANGE TO YOUR OWN DIRECTORY\n","\n","emotions = ['anger', 'fear', 'joy', 'sadness']\n","\n","tweets_train = np.load(join(dataset_directory, 'text_train_tweets.npy'))\n","labels_train = np.load(join(dataset_directory, 'text_train_labels.npy'))\n","vocabulary = load_pickle(join(dataset_directory, 'text_word_to_idx.pkl'))\n","\n","tweets_val = np.load(join(dataset_directory, 'text_val_tweets.npy'))\n","labels_val = np.load(join(dataset_directory, 'text_val_labels.npy'))\n","\n","tweets_test_public = np.load(join(dataset_directory, 'text_test_public_tweets_rand.npy'))\n","\n","tweets_test_private = np.load(join(dataset_directory, 'text_test_private_tweets.npy'))\n","\n","print(len(vocabulary))\n","idx_to_word = {i: w for w, i in vocabulary.items()}\n","for i in range(7):\n","  print(i, idx_to_word[i])\n","\n","sample = 1  ## YOU CAN TRY OUT OTHER TWEETS\n","\n","print('sample tweet, stored form:')\n","print(tweets_train[sample])\n","print(labels_train[sample])\n","\n","print('sample tweet, readable form:')\n","decode = []\n","for i in range(50):\n","  decode.append(idx_to_word[tweets_train[sample][i]])\n","print(decode)\n","print(emotions[labels_train[sample]])\n","\n","\n","print(tweets_train.shape)\n","print(labels_train.shape)\n","print(tweets_val.shape)\n","print(labels_val.shape)\n","print(tweets_test_public.shape)\n","print(tweets_test_private.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","Loaded /content/drive/My Drive/20comp8220/proj/text_dataset/text_word_to_idx.pkl..\n","13978\n","0 <NULL>\n","1 <START>\n","2 <END>\n","3 it\n","4 makes\n","5 me\n","6 so\n","sample tweet, stored form:\n","[ 1 23 24 20 25 19 26 27 28  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0]\n","0\n","sample tweet, readable form:\n","['<START>', 'lol', 'adam', 'the', 'bull', 'with', 'his', 'fake', 'outrage', '<END>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>']\n","anger\n","(7098, 52)\n","(7098,)\n","(1460, 52)\n","(1460,)\n","(4064, 52)\n","(4257, 52)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dk9y7_w_2pTA","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import model_selection,naive_bayes,svm\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qtFYH5HGVPLc"},"source":["The code below performs preproceesing on text datasets."]},{"cell_type":"markdown","metadata":{"id":"-F_8QDhA2pTI","colab_type":"text"},"source":["Libraries for preprocessing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FJHWLg8BVYo7","outputId":"c25666d0-1ecc-469d-b323-43631a20d06a","executionInfo":{"status":"ok","timestamp":1591115287558,"user_tz":-600,"elapsed":1078,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":220}},"source":["import numpy as np\n","from os.path import join\n","from google.colab import drive\n","import pickle\n","import pandas as pd\n","from collections import defaultdict\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import model_selection , naive_bayes , svm\n","from sklearn.metrics import accuracy_score\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"JvdJJ7742pTO","colab_type":"text"},"source":["Managing data to convert tweets into words."]},{"cell_type":"code","metadata":{"id":"KwJ-p8aA2pTQ","colab_type":"code","colab":{}},"source":["def manage_data(raw_tweets):\n","    check ='<'\n","    tweets=[]\n","    for sample in range(len(raw_tweets)):\n","        decode=[]\n","        for i in range(len(raw_tweets[sample])):\n","            decode.append(idx_to_word[raw_tweets[sample][i]])\n","            res = [idx for idx in decode if idx[0].lower() !=check.lower()]\n","        a = \" \".join(res)\n","        tweets.append(a)\n","    return tweets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyRvXpkY2pTX","colab_type":"code","colab":{}},"source":["tweets_train=manage_data(tweets_train)\n","tweets_test_public=manage_data(tweets_test_public)\n","tweets_val=manage_data(tweets_val)\n","tweets_test_private=manage_data(tweets_test_private)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzLwH35x2pTd","colab_type":"text"},"source":["Converting numpy datasets into pandas dataframe and renaming the columns of every dataset."]},{"cell_type":"code","metadata":{"id":"slH61LBW2pTf","colab_type":"code","colab":{}},"source":["tweets_train=pd.DataFrame(tweets_train)\n","tweets_test_public=pd.DataFrame(tweets_test_public)\n","tweets_train=tweets_train.rename(columns={0:'text'})\n","tweets_test_public=tweets_test_public.rename(columns={0:'text'})\n","tweets_val=pd.DataFrame(tweets_val)\n","tweets_val=tweets_val.rename(columns={0:'text'})\n","tweets_train['label']=pd.DataFrame(labels_train)\n","tweets_val['label']=pd.DataFrame(labels_val)\n","tweets_test_private=pd.DataFrame(tweets_test_private)\n","tweets_test_private=tweets_test_private.rename(columns={0:'text'})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZaxweFXs2pTi","colab_type":"text"},"source":["Removing the blank rows, converting the text to lowercase, and, tokenizing the text for training, testing and validation datasets."]},{"cell_type":"code","metadata":{"id":"b2AdVCX92pTj","colab_type":"code","colab":{}},"source":["tweets_train['text'].dropna(inplace =True)\n","tweets_test_public['text'].dropna(inplace =True)\n","tweets_test_private['text'].dropna(inplace =True)\n","tweets_val['text'].dropna(inplace=True)\n","tweets_train['text'] = [entry.lower() for entry in tweets_train['text']]\n","tweets_test_public['text'] = [entry.lower() for entry in tweets_test_public['text']]\n","tweets_test_private['text'] = [entry.lower() for entry in tweets_test_private['text']]\n","tweets_val['text']=[entry.lower() for entry in tweets_val['text'] ]\n","tweets_train['text']= [word_tokenize (entry) for entry in tweets_train['text']]\n","tweets_test_public['text']= [word_tokenize (entry) for entry in tweets_test_public['text']]\n","tweets_test_private['text']= [word_tokenize (entry) for entry in tweets_test_private['text']]\n","tweets_val['text']=[word_tokenize (entry) for entry in tweets_val['text']]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sj_MR80b2pTn","colab_type":"text"},"source":["Part of speech tagging"]},{"cell_type":"code","metadata":{"id":"GvjYyi1F2pTp","colab_type":"code","colab":{}},"source":["tag_map = defaultdict (lambda : wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ex5AFYqC2pTt","colab_type":"text"},"source":["Removing stop words and non alphabetic words, and then, performing lemmatisation for training, testing and validation datasets"]},{"cell_type":"code","metadata":{"id":"T4KvUDep2pTu","colab_type":"code","outputId":"6c64945b-1c1d-47f6-e6ef-a140592b0c6a","executionInfo":{"status":"ok","timestamp":1591115343285,"user_tz":-600,"elapsed":23593,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["for index, entry in enumerate(tweets_train['text']):\n","  final_words = []\n","  word_lemmatized = WordNetLemmatizer() \n","  for word, tag in pos_tag(entry):\n","    if word not in stopwords.words('english') and word.isalpha():\n","      word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]]) \n","      final_words.append(word_final)\n","  tweets_train.loc[index, 'text_final'] = str(final_words)\n","print(tweets_train['text_final'].head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0    ['make', 'fuck', 'irate', 'jesus', 'nobody', '...\n","1           ['lol', 'adam', 'bull', 'fake', 'outrage']\n","2    ['pass', 'away', 'early', 'morning', 'fast', '...\n","3    ['lol', 'wow', 'gon', 'na', 'say', 'really', '...\n","4    ['need', 'sushi', 'date', 'olive', 'guarded', ...\n","Name: text_final, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q-MmTX0m2pT0","colab_type":"code","outputId":"8fbfcdec-978c-4765-9097-bce2e79f6cdf","executionInfo":{"status":"ok","timestamp":1591115359589,"user_tz":-600,"elapsed":11647,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["for index, entry in enumerate(tweets_test_public['text']):\n","  final_words = []\n","  word_lemmatized = WordNetLemmatizer() \n","  for word, tag in pos_tag(entry):\n","    if word not in stopwords.words('english') and word.isalpha():\n","      word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]]) \n","      final_words.append(word_final)\n","  tweets_test_public.loc[index, 'text_final'] = str(final_words)\n","print(tweets_test_public['text_final'].head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0    ['omg', 'mother', 'daughter', 'dull', 'ni', 'm...\n","1    ['happy', 'birthday', 'miss', 'excited', 'back...\n","2    ['ever', 'cry', 'middle', 'bomb', 'rest', 'som...\n","3        ['mentally', 'suffered', 'worthless', 'pain']\n","4    ['courage', 'driver', 'shot', 'bus', 'show', '...\n","Name: text_final, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dv3Djex_2pT5","colab_type":"code","outputId":"8f3705f3-d38d-4a74-8e50-dd2977e8264f","executionInfo":{"status":"ok","timestamp":1591115374231,"user_tz":-600,"elapsed":13073,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["for index, entry in enumerate(tweets_test_private['text']):\n","  final_words = []\n","  word_lemmatized = WordNetLemmatizer() \n","  for word, tag in pos_tag(entry):\n","    if word not in stopwords.words('english') and word.isalpha():\n","      word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]]) \n","      final_words.append(word_final)\n","  tweets_test_private.loc[index, 'text_final'] = str(final_words)\n","print(tweets_test_private['text_final'].head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0    ['whatever', 'decide', 'make', 'sure', 'make',...\n","1    ['accept', 'challenge', 'literally', 'even', '...\n","2    ['roommate', 'okay', 'spell', 'autocorrect', '...\n","3    ['cute', 'atsu', 'probably', 'shy', 'photo', '...\n","4    ['rooneys', 'fuck', 'untouchable', 'fuck', 'dr...\n","Name: text_final, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E5Y2UCVD2pT_","colab_type":"code","outputId":"196526ad-8020-41ed-e264-5c00054d32ac","executionInfo":{"status":"ok","timestamp":1591115381116,"user_tz":-600,"elapsed":5156,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["for index, entry in enumerate(tweets_val['text']):\n","  final_words = []\n","  word_lemmatized = WordNetLemmatizer() \n","  for word, tag in pos_tag(entry):\n","    if word not in stopwords.words('english') and word.isalpha():\n","      word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]]) \n","      final_words.append(word_final)\n","  tweets_val.loc[index, 'text_final'] = str(final_words)\n","print(tweets_val['text_final'].head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0         ['fume', 'hijacked', 'move', 'full', 'back']\n","1                    ['nightmare', 'dream', 'freedom']\n","2    ['cnn', 'really', 'need', 'get', 'business', '...\n","3    ['kikme', 'horny', 'kik', 'nude', 'girl', 'hor...\n","4    ['fuck', 'tag', 'picture', 'family', 'first', ...\n","Name: text_final, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GlCX8esQ2pUD","colab_type":"text"},"source":["Preparing training and testing sets "]},{"cell_type":"code","metadata":{"id":"4B5hYOLT2pUD","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = model_selection.train_test_split(tweets_train['text_final'],tweets_train['label'], test_size=0.3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llfkth6u2pUI","colab_type":"text"},"source":["Encoding"]},{"cell_type":"code","metadata":{"id":"tmtbCcEH2pUK","colab_type":"code","colab":{}},"source":["encoder = LabelEncoder()\n","y_train = encoder.fit_transform(y_train)\n","y_test = encoder.fit_transform(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9sg70kb2pUO","colab_type":"text"},"source":["Using TF-IDF to vectorize words for training, validation and testing sets."]},{"cell_type":"code","metadata":{"id":"JweT06OS2pUO","colab_type":"code","outputId":"f1fae7dd-5a7a-4ff3-8823-a0ebfd75faf4","executionInfo":{"status":"ok","timestamp":1591115392350,"user_tz":-600,"elapsed":1097,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["tfidf_vect = TfidfVectorizer(max_features=5000)\n","tfidf_vect.fit(tweets_train['text_final'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.float64'>, encoding='utf-8',\n","                input='content', lowercase=True, max_df=1.0, max_features=5000,\n","                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n","                smooth_idf=True, stop_words=None, strip_accents=None,\n","                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, use_idf=True, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"R6BBuuLd2pUS","colab_type":"text"},"source":["Performing data vectorization on training, testing and validation datasets."]},{"cell_type":"code","metadata":{"id":"l0GgaRk_2pUT","colab_type":"code","colab":{}},"source":["X_train_tfidf = tfidf_vect.transform(X_train)\n","X_test_tfidf = tfidf_vect.transform(tweets_test_public['text_final'])\n","X_val_tfidf = tfidf_vect.transform(tweets_val['text_final'])\n","X_private_tfidf=tfidf_vect.transform(tweets_test_private['text_final'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gF6wCdyrVX4-"},"source":["The following below is a support vector machine model after preprocessing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uLZku14oVhPQ","scrolled":true,"colab":{}},"source":["from sklearn import model_selection, naive_bayes, svm\n","sv= svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n","\n","# Fit the training dataset.\n","sv.fit(X_train_tfidf, y_train)\n","\n","# Predict the labels on the validation dataset\n","SVtest = sv.predict(X_test_tfidf)\n","SVtrain=sv.predict(X_train_tfidf)\n","SVvalid=sv.predict(X_val_tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPMmU8Tr2pUd","colab_type":"code","outputId":"bfabf233-c93d-4377-974f-d904627fb646","executionInfo":{"status":"ok","timestamp":1591115454978,"user_tz":-600,"elapsed":1742,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(\"Accuracy score\",accuracy_score(SVtrain,y_train))\n","print(\"Accuracy score\",accuracy_score(SVvalid,tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy score 0.964975845410628\n","Accuracy score 0.4438356164383562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WrU4Vm3I2pUi","colab_type":"text"},"source":["# Making a csv file for predictions on public test data"]},{"cell_type":"code","metadata":{"id":"I3ICBUrv2pUj","colab_type":"code","colab":{}},"source":["import csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYheJq622pUn","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/20comp8220/proj/text_dataset/45765758-conv.csv','w') as file:\n","     writer = csv.writer(file)\n","     writer.writerow([\"ID\",\"Prediction\", ])\n","     for i in range(SVtest.shape[0]):\n","\n","       writer.writerow([i+1,SVtest[i]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HEGAmgJbVuSR"},"source":["# Notes on the Conventional ML Model\n","\n","For the final model, we have just chosen the hyperparameters randomly from a given user guide of hyperparameters(https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n","\n","In addition to the final model, I have also tried a random forest model which has performed fairly poorly (accuracy 52.805%). I think this is because unlike SVM, random forest model causes a huge difference between training accuracy score and validation accuracy score which result in overfitting as shown below."]},{"cell_type":"code","metadata":{"id":"RVVOZ9wG2pUq","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","#Create a Gaussian Classifier\n","clf=RandomForestClassifier(n_estimators=100,random_state=123456)\n","\n","\n","clf.fit(X_train_tfidf,y_train)\n","predictions_rand = clf.predict(X_train_tfidf)\n","predictions_train = clf.predict(X_test_tfidf)\n","randomforestpred=clf.predict(X_val_tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"npDWrz-D2pUw","colab_type":"code","outputId":"8be8abca-4393-4f15-8a09-d4c00b6658dd","executionInfo":{"status":"ok","timestamp":1591115507826,"user_tz":-600,"elapsed":915,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(\"Accuracy score->\",accuracy_score(predictions_rand,y_train))\n","print(\"Accuracy score->\",accuracy_score(randomforestpred,tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy score-> 0.9792673107890499\n","Accuracy score-> 0.4280821917808219\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"exJ0WB6EWipX"},"source":["# Deep Learning Model\n","\n","The final model that produced the best-performing predictions for the Kaggle submission (accuracy (54.699)%) is a dense model with two dropout layers which are 0.7 and 0.8.  The input is the training data that has been preprocessed by tfidf,word vectorization, removing stop words and lemmatzation. "]},{"cell_type":"code","metadata":{"id":"VGITGXCN45ug","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4A-yZIR2pU0","colab_type":"code","outputId":"1624a5b4-052e-4586-f80e-4188d41be586","executionInfo":{"status":"ok","timestamp":1591115769406,"user_tz":-600,"elapsed":9661,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from keras.models import Sequential\n","from keras import layers\n","input_dim = X_train_tfidf.shape[1]  # Number of features\n","modeldropout1 = Sequential()\n","modeldropout1.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n","modeldropout1.add(layers.Dense(500,activation='relu'))\n","modeldropout1.add(layers.Dropout(0.7))\n","modeldropout1.add(layers.Dense(700,activation='relu'))\n","modeldropout1.add(layers.Dense(800,activation='relu'))\n","modeldropout1.add(layers.Dropout(0.8))\n","modeldropout1.add(layers.Dense(4, activation='softmax'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GnZKeB1w2pU4","colab_type":"code","outputId":"97b2aa4b-803d-4bf8-8c83-030582c16bba","executionInfo":{"status":"ok","timestamp":1591115783186,"user_tz":-600,"elapsed":11083,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":813}},"source":["modeldropout1.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","                   \n","modeldropout1.fit(X_train_tfidf, y_train,epochs=20, batch_size=128, verbose=1,validation_data=(X_val_tfidf, tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 4968 samples, validate on 1460 samples\n","Epoch 1/20\n","4968/4968 [==============================] - 2s 446us/step - loss: 1.3590 - accuracy: 0.3128 - val_loss: 1.3679 - val_accuracy: 0.2781\n","Epoch 2/20\n","4968/4968 [==============================] - 0s 80us/step - loss: 0.9320 - accuracy: 0.5864 - val_loss: 1.9871 - val_accuracy: 0.3760\n","Epoch 3/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.3101 - accuracy: 0.8979 - val_loss: 2.4757 - val_accuracy: 0.4363\n","Epoch 4/20\n","4968/4968 [==============================] - 0s 79us/step - loss: 0.1266 - accuracy: 0.9632 - val_loss: 2.7150 - val_accuracy: 0.4308\n","Epoch 5/20\n","4968/4968 [==============================] - 0s 77us/step - loss: 0.0933 - accuracy: 0.9716 - val_loss: 2.6698 - val_accuracy: 0.4397\n","Epoch 6/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0722 - accuracy: 0.9738 - val_loss: 2.8223 - val_accuracy: 0.4384\n","Epoch 7/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0690 - accuracy: 0.9720 - val_loss: 3.1515 - val_accuracy: 0.4308\n","Epoch 8/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0609 - accuracy: 0.9726 - val_loss: 2.9445 - val_accuracy: 0.4315\n","Epoch 9/20\n","4968/4968 [==============================] - 0s 79us/step - loss: 0.0552 - accuracy: 0.9750 - val_loss: 3.4428 - val_accuracy: 0.4274\n","Epoch 10/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0516 - accuracy: 0.9750 - val_loss: 3.4985 - val_accuracy: 0.4281\n","Epoch 11/20\n","4968/4968 [==============================] - 0s 77us/step - loss: 0.0468 - accuracy: 0.9730 - val_loss: 4.0163 - val_accuracy: 0.4301\n","Epoch 12/20\n","4968/4968 [==============================] - 0s 81us/step - loss: 0.0434 - accuracy: 0.9758 - val_loss: 4.1149 - val_accuracy: 0.4288\n","Epoch 13/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0422 - accuracy: 0.9783 - val_loss: 4.2588 - val_accuracy: 0.4295\n","Epoch 14/20\n","4968/4968 [==============================] - 0s 80us/step - loss: 0.0470 - accuracy: 0.9754 - val_loss: 4.1972 - val_accuracy: 0.4336\n","Epoch 15/20\n","4968/4968 [==============================] - 0s 77us/step - loss: 0.0413 - accuracy: 0.9748 - val_loss: 4.9591 - val_accuracy: 0.4233\n","Epoch 16/20\n","4968/4968 [==============================] - 0s 78us/step - loss: 0.0413 - accuracy: 0.9781 - val_loss: 4.2221 - val_accuracy: 0.4281\n","Epoch 17/20\n","4968/4968 [==============================] - 0s 79us/step - loss: 0.0374 - accuracy: 0.9783 - val_loss: 4.6233 - val_accuracy: 0.4260\n","Epoch 18/20\n","4968/4968 [==============================] - 0s 77us/step - loss: 0.0391 - accuracy: 0.9801 - val_loss: 5.1889 - val_accuracy: 0.4267\n","Epoch 19/20\n","4968/4968 [==============================] - 0s 79us/step - loss: 0.0421 - accuracy: 0.9760 - val_loss: 4.6090 - val_accuracy: 0.4308\n","Epoch 20/20\n","4968/4968 [==============================] - 0s 77us/step - loss: 0.0391 - accuracy: 0.9771 - val_loss: 5.1525 - val_accuracy: 0.4205\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa9c40d4ba8>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"PmvEsebq2pU7","colab_type":"code","colab":{}},"source":["l=modeldropout1.predict_classes(X_test_tfidf)\n","f=modeldropout1.predict_classes(X_train_tfidf)\n","r=modeldropout1.predict_classes(X_val_tfidf)\n","newprivate=modeldropout1.predict_classes(X_private_tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbM7NliW2pU_","colab_type":"code","outputId":"329c17e0-2c1f-47e6-9b6e-303587afd7e7","executionInfo":{"status":"ok","timestamp":1591115818688,"user_tz":-600,"elapsed":1070,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(\"Accuracy score\",accuracy_score(f,y_train))\n","print(\"Accuracy score\",accuracy_score(r,tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy score 0.9786634460547504\n","Accuracy score 0.42054794520547945\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8OoUKNz3Wv9g"},"source":["[Following this, code and comments as above.]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EcDCI2dIW3ir"},"source":["# Notes on the Deep Learning Model\n","\n","For the final model,I have chosen the hyperparameters randomly. The model, upon running, produces a less difference between validation accuracy score and training accuracy score.\n","\n","In addition to the final model, I have also tried with only dense layers. It has provided an accuracy of 54.379% which is less than neural network with dense layers and dropout. This gap in performance is due to absence of dropout layers. Also, the performance of the model can vary depending on the dataset. It might be that the model with only dense layers can perform better on other datasets, and, the dense model with dropout layers can generalize well on public test set. Furthermore, the model with dense and dropout layers has a better training accuracy score than the model with only dense layers."]},{"cell_type":"markdown","metadata":{"id":"k63EMj3S5vT6","colab_type":"text"},"source":["The following below is the code of neural network model with only dense layers."]},{"cell_type":"code","metadata":{"id":"tsRBR_T_2pVD","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras import layers\n","input_dim = X_train_tfidf.shape[1]  # Number of features\n","model = Sequential()\n","model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n","model.add(layers.Dense(500,activation='relu'))\n","model.add(layers.Dense(800,activation='relu'))\n","model.add(layers.Dense(4, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IuBs3xf2pVH","colab_type":"code","outputId":"5bc2f7ac-442f-40d0-906b-0b5396ba44d5","executionInfo":{"status":"ok","timestamp":1591115852439,"user_tz":-600,"elapsed":8561,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":813}},"source":["model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","                   \n","model.fit(X_train_tfidf, y_train,epochs=20, batch_size=128, verbose=1,validation_data=(X_val_tfidf, tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 4968 samples, validate on 1460 samples\n","Epoch 1/20\n","4968/4968 [==============================] - 0s 100us/step - loss: 1.0673 - accuracy: 0.5320 - val_loss: 2.0121 - val_accuracy: 0.4274\n","Epoch 2/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.1878 - accuracy: 0.9400 - val_loss: 2.2308 - val_accuracy: 0.4356\n","Epoch 3/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0847 - accuracy: 0.9704 - val_loss: 2.8403 - val_accuracy: 0.4199\n","Epoch 4/20\n","4968/4968 [==============================] - 0s 73us/step - loss: 0.0615 - accuracy: 0.9756 - val_loss: 2.8325 - val_accuracy: 0.4363\n","Epoch 5/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0507 - accuracy: 0.9744 - val_loss: 3.3878 - val_accuracy: 0.4370\n","Epoch 6/20\n","4968/4968 [==============================] - 0s 73us/step - loss: 0.0469 - accuracy: 0.9704 - val_loss: 3.0289 - val_accuracy: 0.4390\n","Epoch 7/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0415 - accuracy: 0.9744 - val_loss: 3.3362 - val_accuracy: 0.4370\n","Epoch 8/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0402 - accuracy: 0.9748 - val_loss: 3.2552 - val_accuracy: 0.4370\n","Epoch 9/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0396 - accuracy: 0.9728 - val_loss: 3.5417 - val_accuracy: 0.4185\n","Epoch 10/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0361 - accuracy: 0.9787 - val_loss: 3.3401 - val_accuracy: 0.4432\n","Epoch 11/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0367 - accuracy: 0.9722 - val_loss: 3.4180 - val_accuracy: 0.4315\n","Epoch 12/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0357 - accuracy: 0.9742 - val_loss: 3.8489 - val_accuracy: 0.4315\n","Epoch 13/20\n","4968/4968 [==============================] - 0s 70us/step - loss: 0.0364 - accuracy: 0.9760 - val_loss: 3.4534 - val_accuracy: 0.4315\n","Epoch 14/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0358 - accuracy: 0.9740 - val_loss: 3.7033 - val_accuracy: 0.4342\n","Epoch 15/20\n","4968/4968 [==============================] - 0s 73us/step - loss: 0.0349 - accuracy: 0.9740 - val_loss: 3.6198 - val_accuracy: 0.4281\n","Epoch 16/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0349 - accuracy: 0.9738 - val_loss: 3.6431 - val_accuracy: 0.4349\n","Epoch 17/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0338 - accuracy: 0.9769 - val_loss: 3.8210 - val_accuracy: 0.4288\n","Epoch 18/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0336 - accuracy: 0.9752 - val_loss: 3.9038 - val_accuracy: 0.4247\n","Epoch 19/20\n","4968/4968 [==============================] - 0s 71us/step - loss: 0.0344 - accuracy: 0.9769 - val_loss: 3.9528 - val_accuracy: 0.4370\n","Epoch 20/20\n","4968/4968 [==============================] - 0s 72us/step - loss: 0.0344 - accuracy: 0.9771 - val_loss: 3.8125 - val_accuracy: 0.4274\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa91a766e10>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"zQqGp5Yc2pVK","colab_type":"code","colab":{}},"source":["g=model.predict_classes(X_test_tfidf)\n","h=model.predict_classes(X_train_tfidf)\n","newl=model.predict_classes(X_private_tfidf)\n","f=model.predict_classes(X_val_tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5D2tgiY2pVM","colab_type":"code","outputId":"3c72baaa-a4ba-4414-8968-990b823034ec","executionInfo":{"status":"ok","timestamp":1591115859853,"user_tz":-600,"elapsed":1439,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"","userId":"15453166996577353126"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(\"accuracy score->\",accuracy_score(h,y_train))\n","print(\"accuracy score->\",accuracy_score(f,tweets_val['label']))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracy score-> 0.9768518518518519\n","accuracy score-> 0.4273972602739726\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O1ngpqdsXg2m"},"source":["# Discussion of Model Performance and Implementation\n","\n","Comparing my final conventional ML and deep learning models, I can see that the deep learning one has performed better by 0.32% on the public test set.  The deep learning model has been ranked 35  out of 57 submissions on the public test set. The same model has been ranked 22 out of 49 submissions on the private test with accuracy of 64.153% which is only 9.454% higher than the one which has been submitted on public test set. Though, the model hasn't worked well on the public test set, it has worked better on the private test set. There is nothing wrong with the model. The model is absolutely fine. Depending on the dataset, I am getting different accuracy scores. \n","\t\n"]}]}